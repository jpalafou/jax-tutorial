{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Jax/Numpy API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX on Princeton Research Computing systems\n",
    "\n",
    "https://github.com/PrincetonUniversity/intro_ml_libs/tree/master/jax\n",
    "\n",
    "`pip install -U \"jax[cuda12]\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "\n",
    "try:\n",
    "    import rich\n",
    "except ModuleNotFoundError:\n",
    "    print(\"rich not found, install it with pip install rich\")\n",
    "    !pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "print(f\"{x=}\")\n",
    "print(f\"{type(x)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most numpy functions are available with the `jax.numpy` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.square(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `onp` to convert Jax arrays to Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onp.square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(onp.square(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NumPy, JAX arrays are immutable,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_onp = onp.arange(0.0, 10.0)\n",
    "x_onp[:5] = -1.0\n",
    "x_onp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jnp = jnp.arange(0.0, 10.0)\n",
    "x_jnp[:5] = -1.0\n",
    "x_jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning any modification requires creating a new array rather than altering the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jnp = jnp.arange(0.0, 10.0)\n",
    "x_jnp = x_jnp.at[:5].set(-1.0)\n",
    "x_jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Jax as a tool for computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function\n",
    "\n",
    "$f(x, y, z) = \\sin(x) + e^y + \\sqrt{z}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    x, y, z = X\n",
    "    return jnp.sin(x) + jnp.exp(y) + jnp.sqrt(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has partial derivatives\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = \\cos(x)$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = e^y$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial z} = \\frac{1}{2\\sqrt{z}},$\n",
    "\n",
    "which can be computed exactly with `jax.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdX = grad(f)\n",
    "\n",
    "dfdX(jnp.array([0.0, 0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a parameter $h$:\n",
    "\n",
    "$f(x, y, z; h) = \\sin(x-h) + e^{y-h} + \\sqrt{z-h}.$\n",
    "\n",
    "This gives a new partial derivative\n",
    "\n",
    "$\\frac{\\partial f}{\\partial h} = -\\cos(x-h) - e^{y-h} - \\frac{1}{2\\sqrt{z-h}}.$\n",
    "\n",
    "We can use automatic differentiation to compute $\\frac{\\partial f}{\\partial h}$ by specifying `argnums`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, h):\n",
    "    x, y, z = X\n",
    "    return jnp.sin(x - h) + jnp.exp(y - h) + jnp.sqrt(z - h)\n",
    "\n",
    "\n",
    "grad(f, argnums=1)(jnp.array([0.0, 0.0, 0.25]), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's talk Jacobians and Hessians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacobian, hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a vector-valued function\n",
    "\n",
    "$$\n",
    "\\vec{f}(u, v) =\n",
    "\\left[\\begin{array}{c} \n",
    "f_1(u,v)\\\\\n",
    "f_2(u,v)\n",
    "\\end{array}\\right]=\n",
    "\\left[\\begin{array}{c} \n",
    "e^u \\cos(v)\\\\\n",
    "e^u \\sin(v)\n",
    "\\end{array}\\right].\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fvec(X):\n",
    "    u, v = X\n",
    "    return jnp.array([jnp.exp(u) * jnp.cos(v), jnp.exp(u) * jnp.sin(v)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian of $\\vec{f}$ is written\n",
    "\n",
    "$$\n",
    "\\mathbf{J}_{\\vec{f}}=\n",
    "\\left[\\begin{array}{cc} \n",
    "\\partial f_1 / \\partial u & \\partial f_1 / \\partial v \\\\\n",
    "\\partial f_2 / \\partial u & \\partial f_2 / \\partial v\n",
    "\\end{array}\\right]=\n",
    "\\left[\\begin{array}{cc} \n",
    "e^u \\cos(v) & -e^u \\sin(v)\\\\\n",
    "e^u \\sin(v) & e^u \\cos(v)\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "\n",
    "and can be exactly computed with `jax.jacobian`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fjac = jacobian(fvec)\n",
    "fjac(jnp.array([0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `jax.jacobian` is an alias for `jax.jacrev`, which computes gradients using reverse-mode automatic differentiation. In contrast, `jax.jacfwd` performs forward-mode differentiation. Reverse-mode (`jax.jacrev`) is more efficient for wide matrices, while forward-mode (`jax.jacfwd`) is better suited for tall matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian of $\\vec{f}$,\n",
    "$$\n",
    "\\mathbf{H}_{\\vec{f}}=\n",
    "\\left[\n",
    "\\left[\\begin{array}{cc} \n",
    "\\partial^2 f_1 / \\partial u^2 & \\partial^2 f_1 / \\partial u \\partial v\\\\\n",
    "\\partial^2 f_1 / \\partial v \\partial u & \\partial^2 f_1 / \\partial v^2\n",
    "\\end{array}\\right],\n",
    "\\left[\\begin{array}{cc} \n",
    "\\partial^2 f_2 / \\partial u^2 & \\partial^2 f_2 / \\partial u \\partial v\\\\\n",
    "\\partial^2 f_2 / \\partial v \\partial u & \\partial^2 f_2 / \\partial v^2\n",
    "\\end{array}\\right]\n",
    "\\right]=\n",
    "\\left[\n",
    "\\left[\\begin{array}{cc} \n",
    "e^u \\cos(v) & -e^u \\sin(v)\\\\\n",
    "-e^u \\sin(v) & -e^u \\cos(v)\n",
    "\\end{array}\\right],\n",
    "\\left[\\begin{array}{cc} \n",
    "e^u \\sin(v) & e^u \\cos(v)\\\\\n",
    "e^u \\cos(v) & -e^u \\sin(v)\n",
    "\\end{array}\\right]\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "can be computed exactly with `jax.hessian`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhes = hessian(fvec)\n",
    "fhes(jnp.array([0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Banjamin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX has a much more evolved approach to **random number generation** than NumPy, design to allow parallel random number generators. It is also needed to statisfy the JAX pure function approach.\n",
    "\n",
    "To use any stochastic function in JAX, you need to specify a key, which is a seed that the function can then use internally. So let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42) # Because 42 is the answer\n",
    "print(rng_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1, key2, rng_key = jax.random.split(rng_key,   3)\n",
    "print(key1, key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some random matrices\n",
    "A = jax.random.normal(key1, [500,1000])\n",
    "B = jax.random.normal(key2, [1000, 500])\n",
    "A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX enables operations to execute on CPU/GPU/TPU using the same code thanks to XLA. XLA (Accelerated Linear Algebra) is an open-source compiler for machine learning.\n",
    "\n",
    "When you execute JAX code without JIT, you run through the code at the Python level, until you encounter the low level XLA interface, which is hidden behind the numpy API. At that point, the XLA bit of code is executed, and the result is returned to Python which continues to run through the next commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the original numpy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_np = onp.array(A)\n",
    "B_np = onp.array(B)\n",
    "\n",
    "def func_onp(A, B):\n",
    "  C = onp.dot(A, B)\n",
    "  C = onp.where(C>0, C, 0)\n",
    "  return C\n",
    "\n",
    "\n",
    "%timeit func_onp(A_np, B_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(A, B):\n",
    "  C = jnp.dot(A, B)\n",
    "  C = jnp.where(C>0, C, 0)\n",
    "  return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit func(A, B).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be pretty slow because the execution is still driven by Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of JAX is that you can `jit` a big function, to turn it into a single,compiled XLA graph, that runs without needing Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_func = jax.jit(func) # returns another function\n",
    "%time jitted_func(A, B).block_until_ready(); # First execution won't be fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit jitted_func(A,B).block_until_ready() #  Next calls are fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is based on a **purely functional** approach, with no side effects. A pure function is a function where the output only depends on the inputs of the function.\n",
    "\n",
    "Let's see what we mean by that by creating a simple jitted function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def my_func(x):\n",
    "  print(\"Baguette\")\n",
    "  return 2*jnp.abs(x)\n",
    "\n",
    "\n",
    "y = my_func(0) # The first time I execute it I get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = my_func(1) #  Second time: I see no print!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = my_func(2.0) # Third time: The print is back???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute `grad`, `jit` and `vmap` **with respect to objects**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(cosmo):\n",
    "\n",
    "  return 2* cosmo[\"sigma8\"]**2 + 1\n",
    "\n",
    "cosmo = {\"sigma8\": 0.8, \"omega_m\":0.3}\n",
    "\n",
    "jax.jit(jax.grad(myfunc))(cosmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Matt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very useful functionality of JAX is its automated vectorization with `jax.vmap` \n",
    "Here lets take a function that won't trivially work by just feeding in a 2D array of inputs, a weighted mean function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.array([1.0, 4.0, 0.5])\n",
    "b = jnp.arange(5, 10, dtype=jnp.float32)\n",
    "\n",
    "\n",
    "def weighted_mean(a, b):\n",
    "    output = []\n",
    "    for idx in range(1, b.shape[0] - 1):\n",
    "        output.append(jnp.mean(a + b[idx - 1 : idx + 2]))\n",
    "    return jnp.array(output)\n",
    "\n",
    "\n",
    "print(f\"a shape: {a.shape}\")\n",
    "print(f\"b shape: {b.shape}\")\n",
    "output = weighted_mean(a, b)\n",
    "print(f\"output: {output.shape}\")\n",
    "print(f\"output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see this works as expected with both inputs `a` and `b` as arrays. But what if we wanted to compute this for a large set of 1D arrays? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can use `jax.vmap` to vectorize this calculation without us making any alterations to the function itself! To do this we simply make a stack of $n \\times d$ arrays for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's include the batch dim to the inputs\n",
    "n = 10 # number of elements in the stack\n",
    "stacked_a = jnp.stack([a] * n)\n",
    "stacked_b = jnp.stack([b] * n)\n",
    "\n",
    "# lets show that the input arrays can be different \n",
    "for i in range(n):\n",
    "    stacked_b = stacked_b.at[i].set(b + i)\n",
    "\n",
    "print(f\"stacked_a shape: {stacked_a.shape}\")\n",
    "print(f\"stacked_b shape: {stacked_b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to use our original function this clearly will error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    output = weighted_mean(stacked_a, stacked_a)\n",
    "except:\n",
    "    print(\"Error: the function doesn't take this input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we can use `jax` to perform a vectorization behind the scenes and allow us to call out original function of the stacked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_output = jax.vmap(weighted_mean)(stacked_a, stacked_b)\n",
    "print(f\"stacked output shape: {stacked_output.shape}\")\n",
    "print(f\"stacked output:\")\n",
    "print(stacked_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_one(n, stacked_b, b):\n",
    "    for i in range(n):\n",
    "        stacked_b = stacked_b.at[i].set(b + i)\n",
    "    return stacked_b\n",
    "\n",
    "jit_plus_one = jax.jit(plus_one, static_argnums=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compare the runtimes\n",
    "import time\n",
    "\n",
    "n = [1000, 2000, 4000, 8000, 16000, 32000, 64000]\n",
    "time_loop = []\n",
    "time_vmap = []\n",
    "for iters in n:\n",
    "    start = time.time()\n",
    "    for i in range(iters):\n",
    "        weighted_mean(a, b)\n",
    "    time_loop.append(time.time() - start)\n",
    "\n",
    "    batch_size = iters\n",
    "    batched_a = jnp.stack([a] * batch_size)\n",
    "    batched_b = jnp.stack([b] * batch_size)\n",
    "    # use our jitted plus one function for speed \n",
    "    batched_b = plus_one(batch_size, batched_b, b)\n",
    "    start = time.time()\n",
    "    jax.vmap(weighted_mean)(batched_a, batched_b)\n",
    "    time_vmap.append(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(n, time_loop, label=\"for loop\", alpha=0.5, c=\"firebrick\")\n",
    "plt.scatter(n, time_loop, c=\"firebrick\")\n",
    "plt.plot(n, time_vmap, label=\"vmap\", alpha=0.5, c=\"cornflowerblue\")\n",
    "plt.scatter(n, time_vmap, c=\"cornflowerblue\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"number of function calls\", fontsize=20)\n",
    "plt.ylabel(\"time (s)\", fontsize=20)\n",
    "plt.legend(fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, we can vmap over object/dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo = {'sigma8': jnp.arange(5.), 'omega_m': jnp.arange(5.)}\n",
    "x = 1.\n",
    "def myfunc(dct, x):\n",
    "  return 2*dct['sigma8']**2 + dct['omega_m'] + x\n",
    "out = jax.vmap(myfunc, in_axes=({'sigma8': 0, 'omega_m': None}, None))(cosmo, x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, it means that we are able to turn any function into a vectorized version without edditing the underlying function itself. This works for scalar inputs, but also inputs of vectors and matrices, just add a new blank dimension to the start of the object, and stack them up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want more speed? Well `jax` can also distribute tasks automatically across devices. Let us see what devices we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what device our object is attached to currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = jnp.arange(32.0).reshape(4, 8)\n",
    "arr.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a cool tool to visualise the partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all the data is on one device for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But here we can use `jax.sharding` to allocate different devices to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec as P\n",
    "\n",
    "n = jax.device_count()\n",
    "print(f\"Sharding overs {n} devices\")\n",
    "\n",
    "mesh = jax.make_mesh((n, 1), (\"x\", \"y\"))\n",
    "sharding = jax.sharding.NamedSharding(mesh, P(\"x\", \"y\"))\n",
    "print(sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see where our data is held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_sharded = jax.device_put(arr, sharding)\n",
    "\n",
    "print(arr_sharded)\n",
    "jax.debug.visualize_array_sharding(arr_sharded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use JIT to let the XLA compilers in JAX perform the optimal load management and run away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f_contract(x):\n",
    "    return x.sum(axis=0)\n",
    "\n",
    "\n",
    "result = f_contract(arr_sharded)\n",
    "jax.debug.visualize_array_sharding(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
